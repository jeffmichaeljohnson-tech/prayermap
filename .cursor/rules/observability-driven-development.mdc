---
description: Observability-Driven Development (ODD) protocol - when and how to use Datadog
globs:
  - "**/*"
alwaysApply: true
---

# Observability-Driven Development (ODD) Protocol

> **Core Principle**: Data-driven decisions beat intuition. Check metrics before guessing, verify after changes, and debug with evidence.

---

## ğŸš¨ MANDATORY Datadog Checks

### When You MUST Check Datadog

**1. Before Debugging Any Error** âš ï¸ CRITICAL
```
âŒ WRONG: "I see an error, let me check the code"
âœ… RIGHT: "I see an error, let me check Datadog first to understand the context"
```

**Why**: Errors in Datadog show frequency, trends, user context, related errors, and performance impact.

**Action**: 
1. Go to Datadog â†’ Logs â†’ Search for error message
2. Check error frequency and trends
3. Review error context (user_id, component, etc.)
4. Look for related errors
5. THEN investigate code

---

**2. Before Performance Work** âš ï¸ CRITICAL
```
âŒ WRONG: "This feels slow, let me optimize"
âœ… RIGHT: "Let me check Datadog to see actual performance metrics"
```

**Why**: Performance work without metrics is guessing.

**Action**:
1. Check Datadog â†’ APM â†’ Service Performance
2. Identify slowest operations
3. Review query performance (PostgreSQL)
4. Check animation FPS metrics
5. THEN optimize based on data

---

**3. After Making Changes** âš ï¸ CRITICAL
```
âŒ WRONG: "Code compiles, I'm done"
âœ… RIGHT: "Code compiles, let me verify metrics show improvement"
```

**Why**: Changes can have unintended consequences.

**Action**:
1. Make change
2. Wait 2-5 minutes for metrics to update
3. Check Datadog for error rate changes, performance changes, new errors
4. Verify improvement or investigate regression

---

**4. When Investigating User Reports** âš ï¸ CRITICAL
```
âŒ WRONG: "User says it's broken, let me check code"
âœ… RIGHT: "User says it's broken, let me check Datadog for that user/timeframe"
```

**Why**: User reports need context - is it affecting all users or just one?

**Action**:
1. Check Datadog â†’ Logs â†’ Filter by user_id or timeframe
2. Review error patterns
3. Check performance metrics for that period
4. Look for related issues
5. THEN investigate code

---

## ğŸ” OPTIONAL Datadog Checks

### When Checking is Helpful But Not Required

**1. Simple Refactors** (No behavior change)
- If no behavior changes, metrics shouldn't change
- Still good to verify, but not critical

**2. Documentation Updates**
- No code changes = no metric changes
- Skip unless documenting metrics themselves

**3. Trivial Bug Fixes** (Obvious issues)
- If error is obvious (typo, null check), fix first
- Then verify metrics show improvement

**4. Styling/CSS Changes**
- Visual changes don't affect backend metrics
- Still check animation/layout metrics if relevant

---

## ğŸ› Auto-Enable Debug Mode Protocol

### When to Enable Debug Mode

**MANDATORY**: Enable debug mode on **first failure**:

```typescript
// In your code or agent workflow:
if (errorOccurred && !debugModeEnabled) {
  enableDebugMode();
  logToDatadog('Debug mode enabled due to error', { error, context });
}
```

**Why**: 
- First failure might be a fluke
- Second failure = pattern = need more data
- Debug mode provides detailed context

**Action**:
1. **First error**: Enable debug mode automatically
2. **Check Datadog**: Review detailed logs
3. **If error persists**: Debug mode already on, investigate
4. **After fix**: Disable debug mode (or leave on for 24h)

---

## ğŸ“Š What to Check in Datadog

### For Errors
- Error frequency: How often does it occur?
- Error trends: Is it increasing/decreasing?
- Error context: user_id, component, action
- Related errors: Are there patterns?
- Performance impact: Is it slowing things down?

**Location**: Datadog â†’ Logs â†’ Search error message

---

### For Performance
- Query performance: PostgreSQL slow queries
- API latency: Endpoint response times
- Animation FPS: Frame rate metrics
- Message latency: Realtime delivery times
- Layout shifts: CLS scores

**Location**: Datadog â†’ APM â†’ Service Performance

---

### For Messaging/Realtime
- Connection health: % connected vs disconnected
- Message latency: p50, p95, p99 delivery times
- Subscription errors: Failed subscriptions
- Reconnection rate: How often connections drop

**Location**: Datadog â†’ Logs â†’ Search "realtime"

---

### For Animations
- FPS: Average frame rate (target: 60fps)
- Jank rate: % frames >16.67ms (target: <10%)
- Completion variance: Actual vs expected duration
- Layout shifts: CLS score (target: <0.1)

**Location**: Datadog â†’ Logs â†’ Search "animation"

---

## ğŸ¯ Agent Workflow Integration

### Standard Agent Workflow with ODD

```
1. RECEIVE TASK
   â†“
2. CHECK DATADOG (if debugging/performance/errors)
   â”œâ”€ Review relevant metrics
   â”œâ”€ Understand baseline
   â””â”€ Identify patterns
   â†“
3. RESEARCH (query memory, check docs)
   â†“
4. IMPLEMENT
   â†“
5. VERIFY IN DATADOG (mandatory after changes)
   â”œâ”€ Check error rates
   â”œâ”€ Check performance metrics
   â””â”€ Verify improvement
   â†“
6. DOCUMENT FINDINGS
```

---

### Error Handling Workflow

```
1. ERROR OCCURS
   â†“
2. AUTO-ENABLE DEBUG MODE (first failure)
   â†“
3. CHECK DATADOG IMMEDIATELY
   â”œâ”€ Search error message
   â”œâ”€ Check frequency/trends
   â”œâ”€ Review context
   â””â”€ Identify patterns
   â†“
4. INVESTIGATE CODE (with context from Datadog)
   â†“
5. FIX
   â†“
6. VERIFY IN DATADOG
   â”œâ”€ Error should stop
   â”œâ”€ Performance should improve
   â””â”€ No new errors
   â†“
7. DISABLE DEBUG MODE (after 24h if stable)
```

---

## ğŸš« When NOT to Check Datadog

**Don't check for**:
- Simple typos or syntax errors
- Documentation-only changes
- Trivial refactors with no behavior change
- Styling changes (unless animation-related)
- Initial setup/installation

**Use judgment**: If checking Datadog would take longer than fixing the issue, skip it.

---

## ğŸ’¡ Best Practices

### 1. Check Before Guessing
```
âŒ "This might be slow because..."
âœ… "Datadog shows this is slow because..."
```

### 2. Verify After Changes
```
âŒ "I fixed it"
âœ… "I fixed it, and Datadog confirms error rate dropped from X to Y"
```

### 3. Enable Debug Early
```
âŒ "Let me try a few things first"
âœ… "First error = enable debug mode, then investigate"
```

### 4. Use Metrics to Prioritize
```
âŒ "This feels important"
âœ… "Datadog shows this affects 50% of users, so it's high priority"
```

### 5. Document Findings
```
âœ… "Datadog showed error rate of X before fix, Y after fix"
âœ… "Performance improved from Xms to Yms according to Datadog"
```

---

## ğŸ“š Full Documentation

See `docs/OBSERVABILITY_DRIVEN_DEVELOPMENT.md` for complete protocol with examples.

---

**Remember**: Data-driven decisions beat intuition. Check metrics before guessing, verify after changes, and debug with evidence.
